# -*- coding: utf-8 -*-
"""Сборный проект по оттоку сотрудников

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FCQdcWSAHEy8TJmMir98xCv5hkEEvcGT

# Сборный проект по оттоку сотрудников

***
**Описание проекта:** Удовлетворённость работой напрямую влияет на отток сотрудников. А предсказание оттока — одна из важнейших задач HR-аналитиков. Внезапные увольнения несут в себе риски для компании, особенно если уходит важный сотрудник.

**Цель:** Разработать два прогностических алгоритма на основе методов машинного обучения:

- модель для предсказания уровня удовлетворённости сотрудника работой;

- модель для предсказания вероятности увольнения сотрудника

**Ход исследования:**
Шаг 1. Загрузка и изучение данных: загрузить датасеты и изучить структуру данных
Шаг 2. Предобработка данных: обработка пропусков
Шаг 3. Исследовательский анализ данных: статистический анализ всех признаков и построение графиков
Шаг 4. Подготовка данных
Шаг 5. Обучение моделей
Шаг 6. Выводы
Шаг 7. Повторение предыдущих действий для второй задачи и добавление нового входного признака
Шаг 8. Итоговые выводы

**Общий вывод:** резюмирование полученных результатов, формулировка ключевых выводов и рекомендаций.

С помощью данного исследования мы стремимся дать всесторонний анализ удовлетворенности сотрудников, что станет основой HR-аналитики, для прослеживания потребностей работников и помощи в ситуациях близких к увольнению.
В условиях высокой конкуренции на рынке труда и растущей значимости человеческого капитала для достижения стратегических целей компании, управление персоналом становится ключевым направлением бизнеса. Потеря квалифицированных сотрудников ведёт к прямым и косвенным затратам: расходы на найм и обучение, потеря производительности, снижение мотивации команды.

# Задача 1: предсказание уровня удовлетворённости сотрудника
"""

from google.colab import files
uploaded = files.upload()

!pip install phik

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import shap
import phik
import time
from sklearn.metrics import make_scorer

from sklearn.pipeline import Pipeline
from sklearn.model_selection import train_test_split, RandomizedSearchCV, GridSearchCV
from sklearn.linear_model import LinearRegression, LogisticRegression
from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor
from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler, MinMaxScaler, LabelEncoder
from sklearn.ensemble import RandomForestRegressor

from sklearn.metrics import confusion_matrix, classification_report, f1_score, roc_auc_score, mean_absolute_error, mean_squared_error

from sklearn.impute import SimpleImputer
from sklearn.svm import SVC

plt.rcParams["figure.figsize"] = (7, 7)

RANDOM_STATE = 42
TEST_SIZE = 0.25

"""## Шаг. Загрузка данных

### Создание датафреймов train_job, test_features, test_target
"""

train_job = pd.read_csv('train_job_satisfaction_rate.csv')
test_features = pd.read_csv('test_features.csv')
test_target = pd.read_csv('test_target_job_satisfaction_rate.csv')

def analysis_df (data):
    data.info()
    display(data.isna().sum())
    display(data.duplicated().sum())
    display(data.head(10))
    display(data.shape)

analysis_df(train_job)

analysis_df(test_features)

analysis_df(test_target)

"""### Выводы по каждому датафрейму

Тренировочная выборка:
train_job: 4000 строк, 10 столбцов, есть пропуски в двух столбцах - 10 штук, явных дубликатов нет, типы данных совпадают с данными в столбцах

Входные признаки тестовой выборки:
test_features: 2000 строк, 9 столбцов, 3 пропуска суммарно в двух столбцах, явных дубликатов нет, типы данных совпадают с данными в столбцах

Целевой признак тестовой выборки:
test_target: 2000 строк, 2 столбца, нет пропусков и явных дубликатов, типы данных совпадают с данными в столбцах

## Шаг. Предобработка данных
### Поиск неявных дубликатов
"""

train_job['dept'].unique()

train_job['level'].unique()

train_job['workload'].unique()

train_job['last_year_promo'].unique()

train_job['last_year_violations'].unique()

test_features['dept'].unique()

test_features['dept'] = test_features['dept'].replace(' ', np.nan)

test_features['level'].unique()

test_features['workload'].unique()

test_features['workload'] = test_features['workload'].replace(' ', np.nan)

test_features['last_year_promo'].unique()

test_features['last_year_violations'].unique()

"""Обнаружили еще 2 пропущенных значения, сразу заменили их на nan, чтобы далее их убрать из датафрейма, так как количество очень мало 10/4000 и 5/2000.

Проверили уникальные значения в категориальных столбцах, неявных дубликатов не обнаружено, все уникальные значения в единственном экземпляре

### Удаление пропусков
"""

train_job = train_job.dropna()

train_job.info()

test_features.info()

"""### Вывод

На предыдущем шаге сразу выявили совпадения типов данных, отстутствие явны дубликатов, пропуски в некоторых датафреймах, которые удалили за ненадобностью, изучили уникальные значения для проверки неявных дубликатов.

## Шаг. Исследовательский анализ данных
### Изучение значений количественных столбцов
"""

train_job.drop(columns='id').describe()

test_features.drop(columns='id').describe()

test_target.drop(columns='id').describe()

"""Явных выбросов в данных не видно, построим гистограммы для уверенности

### Гистограммы для количественных переменных
"""

num_cols = ['salary', 'job_satisfaction_rate']

for col in num_cols:
    plt.figure()
    train_job[col].hist(bins=5)
    plt.title(f'Распределение {col}')
    plt.xlabel(col)
    plt.ylabel('Количество')
    plt.grid(True);

test_features['salary'].hist(bins=5)
plt.title(f'Распределение "salary"')
plt.xlabel('Уровень зарплаты')
plt.ylabel('Количество')
plt.grid(True);

test_target['job_satisfaction_rate'].hist(bins=20)
plt.title(f'Распределение уровня удовлетворённости сотрудника работой в компании')
plt.xlabel('Уровень удовтлетворенности')
plt.ylabel('Количество')
plt.grid(True);

"""Не все данные нормально распределены, явных выбросов также не видно.

### Барплоты для качественных переменных
"""

cat_cols = ['dept', 'level', 'workload', 'last_year_promo', 'last_year_violations', 'employment_years', 'supervisor_evaluation', ]

for col in cat_cols:
    plt.figure()
    sns.countplot(data=train_job, x=col, order=train_job[col].value_counts().index)
    plt.title(f'Распределение по {col}')
    plt.xlabel(col)
    plt.ylabel('Количество');

cat_cols = ['dept', 'level', 'workload', 'last_year_promo', 'last_year_violations', 'employment_years', 'supervisor_evaluation', ]

for col in cat_cols:
    plt.figure()
    sns.countplot(data=test_features, x=col, order=test_features[col].value_counts().index)
    plt.title(f'Распределение по {col}')
    plt.xlabel(col)
    plt.ylabel('Количество');

"""По данным как в тренировочной, так и в тестовой выборке самый популярный отдел, в котором работает сотрудник - продажи(sales)

Большинство работает на junior уровне

Уровень загруженности сотрудника - средний

У большинства не было повышения за последний год

Большинство сотрудников не нарушало трудовой договор за последний год;

### Матрица корреляции
"""

train_job_drop = train_job.drop('id', axis=1)
numeric_market = train_job_drop.select_dtypes(include='number')

corr_new = train_job_drop.phik_matrix(interval_cols=numeric_market)

plt.figure(figsize=(15,10))
sns.heatmap(corr_new, annot=True, cmap='coolwarm', fmt=".2f")
plt.title('Корреляционная матрица train_job')
plt.show()

test_features_drop = test_features.drop('id', axis=1)
numeric_market = test_features_drop.select_dtypes(include='number')

corr_new = test_features_drop.phik_matrix(interval_cols=numeric_market)

plt.figure(figsize=(15,10))
sns.heatmap(corr_new, annot=True, cmap='coolwarm', fmt=".2f")
plt.title('Корреляционная матрица test_features')
plt.show()

"""Подводя итог по корреляционной матрице можно сделать вывод о том, что данные тестовой и тренировочной выборки схожи, мультиколинеарности нет, но есть зависимость признаков:
- level и salary
- workload и salary
- employment_years и salary
- employment_years и level
- workload и level
"""

train_job.head()

test_features.head()

train_copy = train_job.copy()
test_copy = test_features.copy()
train_copy['data'] = 'train'
test_copy['data'] = 'test'

combined = pd.concat([train_copy, test_copy], axis=0, ignore_index=True)

cols = ['salary']
cat_cols = ['dept', 'level', 'workload', 'last_year_promo', 'last_year_violations', 'employment_years', 'supervisor_evaluation']


for col in cols:
    plt.figure()
    sns.histplot(
        data=combined, x=col, hue='data', multiple='dodge', common_norm=False
    )
    plt.title(f'Сравнение распределения "{col}" (train vs test)')
    plt.xticks(rotation=45)
    plt.tight_layout()
    plt.show()

for col in cat_cols:
    plt.figure()
    sns.countplot(data=combined, x=col, hue='data', order=combined[col].value_counts().index)
    plt.title(f'Распределение по {col}')
    plt.xlabel(col)
    plt.ylabel('Количество');

"""### Вывод

- Изучили значения в количественных перменых, выбросов не нашли, данные не все равномерно распределены

- Построили гистограммы для количсетвенных признаков

- Построили барплоты для категориальных признаков

- Построили матрицу корреляции и оценили результаты

## Шаг. Подготовка данных
### Подготовка пайплайна
"""

X_train = train_job.drop(columns='job_satisfaction_rate')
y_train = train_job['job_satisfaction_rate']

test_final = test_features.merge(test_target, on='id')

X_test = test_final.drop(columns='job_satisfaction_rate')
y_test = test_final['job_satisfaction_rate']

y_test = y_test.loc[X_test.index]

num_columns = ['salary', 'supervisor_evaluation', 'employment_years']
ohe_columns = ['dept', 'last_year_promo', 'last_year_violations']
ord_columns = ['level', 'workload']

ohe_pipe = Pipeline(
    [('simpleImputer_ohe', SimpleImputer(missing_values=np.nan, strategy='most_frequent')),
     ('ohe', OneHotEncoder(drop='first', handle_unknown='ignore', sparse_output=False))
    ]
    )

ord_pipe = Pipeline(
    [('simpleImputer_before_ord', SimpleImputer(missing_values=np.nan, strategy='most_frequent')),
     ('ord',  OrdinalEncoder(
                categories=[['junior', 'middle', 'sinior'],
                            ['low', 'medium', 'high'],
                           ],
                handle_unknown='use_encoded_value',
                unknown_value=np.nan
            )
        ),
     ('simpleImputer_after_ord', SimpleImputer(missing_values=np.nan, strategy='most_frequent'))
    ]
)

data_preprocessor = ColumnTransformer(
    [('ohe', ohe_pipe, ohe_columns),
     ('ord', ord_pipe, ord_columns),
     ('num', MinMaxScaler(), num_columns)
    ],
    remainder='passthrough'
)

"""### Вывод

Подготовили данные в пайплайне для дальнейшей работы

## Шаг. Обучение моделей
### Новая метрика smape
"""

def smape(y_true, y_pred):
    y_true = np.array(y_true)
    y_pred = np.array(y_pred)

    delt = (np.abs(y_true) + np.abs(y_pred)) / 2
    nonzero = delt != 0

    smape_values = np.abs(y_pred[nonzero] - y_true[nonzero]) / delt[nonzero]
    return np.mean(smape_values) * 100

smape_scorer = make_scorer(smape, greater_is_better=False)

"""Оформили новую метрику smape в функцию, исключили деление на ноль

### Пайплайн с обучением моделей
"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# pipe_final = Pipeline([
#     ('preprocessor', data_preprocessor),
#     ('models', LinearRegression())
# ])
# 
# param_grid = [
#     {
#         'models': [DecisionTreeRegressor(random_state=RANDOM_STATE)],
#         'models__max_depth': range(6, 16),
#         'models__max_features': range(7, 17),
#         'preprocessor__num': [StandardScaler(), MinMaxScaler()]
#     },
#     {
#         'models': [LinearRegression()],
#         'preprocessor__num': [StandardScaler(), MinMaxScaler()]
#     },
#     {
#         'models': [RandomForestRegressor(random_state=RANDOM_STATE)],
#         'models__n_estimators': [50, 100],
#         'models__max_depth': [5, 10],
#         'models__max_features': ['sqrt', 'log2'],
#         'preprocessor__num': [StandardScaler(), MinMaxScaler()]
#     }
# ]
# 
# grid_search = GridSearchCV(
#     pipe_final,
#     param_grid,
#     cv=5,
#     scoring=smape_scorer,
#     n_jobs=-1,
#     error_score='raise'
# )
# grid_search.fit(X_train, y_train)
# 
# print('Лучшая модель и её параметры:\n\n', grid_search.best_estimator_)

"""Протестировала различное кол-во моделей в обучении - лучшей стала DecisionTreeRegressor(max_depth=11, max_features=12, random_state=42) с этими значениями наибольший процент правильных решений."""

cv_results_df = pd.DataFrame(grid_search.cv_results_)

cv_results_df.head()

grid_search.best_score_

y_pred_test = grid_search.best_estimator_.predict(X_test)
smape(y_test, y_pred_test)

"""Получили результат 14.94, что < 15, значит модель работает действительно хорошо. Для проверки воспользуемся еще несколькими метриками."""

model = grid_search.best_estimator_
y_pred = model.predict(X_test)

mean_absolute_error(y_test, y_pred)

mse = mean_squared_error(y_test, y_pred)

rmse = np.sqrt(mse)

"""### Вывод

MAE: 0.06 — в среднем предсказания отклоняются от фактических значений на ~0.06 единиц, что говорит о высокой точности модели.

MSE: 0.006 — ошибка небольшая, модель не допускает крупных промахов.

RMSE: 0.08 — также указывает на хорошее качество предсказаний.

Итог: Модель обучена качественно, ошибок немного, сильных выбросов нет.

## Шаг. Оформление выводов
### Выводы

DecisionTreeRegressor (max\_depth=11, max\_features=12)

***

Модель хорошо захватила нелинейные зависимости в данных, чего не смогла сделать линейная регрессия.

Значение подобрано оптимально — дерево при построении в большем масштабе давала переобучение, поэтому 12/12 стало оптимальным вариантом, это дало более точные предсказания.

SMAPE и другие метрики (r2, MAE, MSE, RMSE) показали, что дерево подстроилось сильнее линейной регрессии.

DecisionTreeRegressor оказался лучшим выбором, потому что умеет выявлять сложные зависимости в данных, при этом дерево сохраняет хорошую обобщающую способность на тестовой выборке.

# Задача 2: предсказание увольнения сотрудника из компании
## Шаг. Загрузка данных
### Загрузка датафреймов train_quit, test_features, test_target_quit
"""

train_quit = pd.read_csv('train_quit.csv')
test_features = pd.read_csv('test_features.csv')
test_target_quit = pd.read_csv('test_target_quit.csv')

"""### Проверка данных с функцией"""

analysis_df(train_quit)

analysis_df(test_features)

analysis_df(test_target_quit)

"""### Вывод

У нас есть 3 датафрейма:
    
Тренировочная выборка: train_quit - 4000 строк, 10 столбцов, нет пропусков, данные в порядке, типы данных соответсвуют, дубликатов нет

Входные признаки тестовой выборки те же, что и в прошлой задаче: test_features - 2000 строк, 9 столбцов, есть 3 пропуска, дубликатов нет, данные в порядке, типы данных соответствуют

Целевой признак тестовой выборки: test_target_quit - 2000 строк, 2 столбца, пропусков нет, данные в порядке, типы данных соответсвуют, дубликатов нет

## Шаг. Предобработка данных
### Поиск неявных дубликатов
"""

train_quit.head()

train_quit['dept'].unique()

train_quit['level'].unique()

train_quit['workload'].unique()

train_quit['last_year_promo'].unique()

train_quit['last_year_violations'].unique()

test_features['dept'].unique()

test_features['dept'] = test_features['dept'].replace(' ', np.nan)

test_features['level'].unique()

test_features['workload'].unique()

test_features['workload'] = test_features['workload'].replace(' ', np.nan)

test_features['last_year_promo'].unique()

test_features['last_year_violations'].unique()

test_target_quit['quit'].unique()

"""Убрали пустые значени, удалили пропуски, дополнительно проанализировали уникальные значения для поиска неявных дубликатов

### Изучение количественных признаков
"""

train_quit.drop(columns='id').describe()

test_features.drop(columns='id').describe()

"""### Вывод

Не нашли неявных дубликатов, типы данных в порядке, выбросов не видно, пайплайн перемещен к основному в след. пункте для удобства.

## Шаг. Исследовательский анализ данных
### Исследовательский анализ даных
"""

num_cols = ['employment_years', 'supervisor_evaluation', 'salary']

for col in num_cols:
    plt.figure()
    train_quit[col].hist(bins=5)
    plt.title(f'Распределение {col}')
    plt.xlabel(col)
    plt.ylabel('Количество')
    plt.grid(True);

for col in num_cols:
    plt.figure()
    test_features[col].hist(bins=5)
    plt.title(f'Распределение {col}')
    plt.xlabel(col)
    plt.ylabel('Количество')
    plt.grid(True);

cat_cols = ['dept', 'level', 'workload', 'last_year_promo', 'last_year_violations', 'quit']

for col in cat_cols:
    plt.figure()
    sns.countplot(data=train_quit, x=col, order=train_quit[col].value_counts().index)
    plt.title(f'Распределение по {col}')
    plt.xlabel(col)
    plt.ylabel('Количество');

cat_cols = ['dept', 'level', 'workload', 'last_year_promo', 'last_year_violations']

for col in cat_cols:
    plt.figure()
    sns.countplot(data=test_features, x=col, order=test_features[col].value_counts().index)
    plt.title(f'Распределение по {col}')
    plt.xlabel(col)
    plt.ylabel('Количество');

sns.countplot(data=test_target_quit, x='quit', order=test_target_quit['quit'].value_counts().index)
plt.title('Распределение по quit')
plt.xlabel('quit')
plt.ylabel('Количество');

"""Проведен исследовательский анализ данных, изучены количественные и качественные признаки,в целевом признаке - бинарная классификация(неравномерна)"""

train_quit_drop = train_quit.drop('id', axis=1)
numeric_market = train_quit_drop.select_dtypes(include='number')

corr_new = train_quit_drop.phik_matrix(interval_cols=numeric_market)

plt.figure(figsize=(15,10))
sns.heatmap(corr_new, annot=True, cmap='coolwarm', fmt=".2f")
plt.title('Корреляционная матрица train_job')
plt.show()

"""По матрице корреляции видим, что мультиколинеарности нет, но сильная взаимосвязь между:
- salary, level, workload, employment_years, quit
"""

train_quit.groupby('quit')

cat_cols = train_quit.select_dtypes(include='object').drop(columns='quit').columns
num_cols = train_quit.select_dtypes(include='number').columns

for col in num_cols:
    plt.figure()
    sns.histplot(data=train_quit, x=col, hue='quit', common_norm=False, bins=5, kde=True)
    plt.title(f'Распределение {col} по quit')
    plt.xlabel(col)
    plt.ylabel('Количество');

for col in cat_cols:
    plt.figure()
    sns.countplot(data=train_quit, x=col, hue='quit', order=train_quit[col].value_counts().index)
    plt.title(f'Распределение {col} по quit')
    plt.xlabel(col)
    plt.ylabel('Количество')
    plt.legend(title='quit');

y_pred_job = model.predict(train_quit)

train_quit['job_satisfaction_rate'] = y_pred_job

train_quit.head()

plt.figure()
sns.histplot(
    data=train_quit,
    x='job_satisfaction_rate',
    hue='quit',
    common_norm=False,
)
plt.title('Распределение удовлетворённости работой')
plt.xlabel('Уровень удовлетворённости сотрудников')
plt.ylabel('Количество')
plt.grid(True);

"""Портрет «уволившегося сотрудника»:
    
- в отделе 'sales' и 'tecnology' с большей вероятностью работает уволившийся сотрудник и у него уровень загруженности средний или низкий.
    
- если рассматривать зарпалты ушедших сотрудников - то боьшинство уходит с зарплатой до 30 тысяч.

## Шаг. Добавление нового входного признака
Признак добавлен на предыдущем шаге в тренировочные данные, здесь добавлени и в тестовую выборку.
"""

y_pred_test = model.predict(test_features)

test_features['job_satisfaction_rate'] = y_pred_test

test_features.head()

"""## Шаг. Подготовка данных"""

train_quit.head()

X_train = train_quit.drop(columns='quit')
y_train = train_quit['quit']

test_features.head()

test_final = test_features.merge(test_target_quit, on='id')

X_test = test_final.drop(columns='quit')
y_test = test_final['quit']

y_test = y_test.loc[X_test.index]

num_columns = ['employment_years', 'supervisor_evaluation', 'salary', 'job_satisfaction_rate']
ohe_columns = ['dept', 'last_year_promo', 'last_year_violations']
ord_columns = ['level', 'workload']

label_encoder = LabelEncoder()

y_train_encoded = label_encoder.fit_transform(y_train)

ohe_pipe = Pipeline(
    [('simpleImputer_ohe', SimpleImputer(missing_values=np.nan, strategy='most_frequent')),
     ('ohe', OneHotEncoder(drop='first', handle_unknown='ignore', sparse_output=False))
    ]
    )

ord_pipe = Pipeline(
    [('simpleImputer_before_ord', SimpleImputer(missing_values=np.nan, strategy='most_frequent')),
     ('ord',  OrdinalEncoder(
                categories=[['junior', 'middle', 'sinior'],
                            ['low', 'medium', 'high'],
                           ],
                handle_unknown='use_encoded_value',
                unknown_value=np.nan
            )
        ),
     ('simpleImputer_after_ord', SimpleImputer(missing_values=np.nan, strategy='most_frequent'))
    ]
)

data_preprocessor = ColumnTransformer(
    [('ohe', ohe_pipe, ohe_columns),
     ('ord', ord_pipe, ord_columns),
     ('num', MinMaxScaler(), num_columns)
    ],
    remainder='passthrough'
)

"""## Шаг. Обучение модели"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# ohe_pipe = Pipeline([
#     ('simpleImputer_ohe', SimpleImputer(missing_values=np.nan, strategy='most_frequent')),
#     ('ohe', OneHotEncoder(drop='first', handle_unknown='ignore', sparse_output=False))
# ])
# 
# ord_pipe = Pipeline([
#     ('simpleImputer_before_ord', SimpleImputer(missing_values=np.nan, strategy='most_frequent')),
#     ('ord', OrdinalEncoder(
#         categories=[['junior', 'middle', 'sinior'],
#                     ['low', 'medium', 'high']],
#         handle_unknown='use_encoded_value',
#         unknown_value=np.nan
#     )),
#     ('simpleImputer_after_ord', SimpleImputer(missing_values=np.nan, strategy='most_frequent'))
# ])
# 
# data_preprocessor = ColumnTransformer(
#     [('ohe', ohe_pipe, ohe_columns),
#      ('ord', ord_pipe, ord_columns),
#      ('num', MinMaxScaler(), num_columns)
# ], remainder='drop')
# 
# pipe_final = Pipeline([
#     ('preprocessor', data_preprocessor),
#     ('models', LogisticRegression(random_state=42))
# ])
# 
# param_grid = [
#     {
#         'models': [DecisionTreeClassifier(random_state=42)],
#         'models__max_depth': range(5, 7),
#         'models__max_features': range(7, 13),
#         'preprocessor__num': [StandardScaler(), MinMaxScaler()]
#     },
#     {
#         'models': [KNeighborsClassifier()],
#         'models__n_neighbors': range(3, 8),
#         'preprocessor__num': [StandardScaler(), MinMaxScaler()]
#     },
#     {
#         'models': [LogisticRegression(
#             random_state=RANDOM_STATE,
#             solver='liblinear',
#             penalty='l1'
#         )],
#         'models__C': range(1,9),
#         'preprocessor__num': [StandardScaler(), MinMaxScaler()]
#     }
# ]
# grid_search = GridSearchCV(
#     pipe_final,
#     param_grid,
#     cv=5,
#     scoring='roc_auc',
#     n_jobs=-1,
#     error_score='raise'
# ).fit(X_train, y_train)
# 
# display('Лучшая модель и её параметры:\n\n', grid_search.best_estimator_)

model_two = grid_search.best_estimator_

y_pred_proba = model_two.predict_proba(X_test)[:, 1]

roc_auc_score(y_test, y_pred_proba)

"""### Вывод

KNeighborsClassifier(n_neighbors=7) показала себя лучшей в нашем пайплайне.

## Шаг. Выводы
Лучшая модель:
KNeighborsClassifier(n_neighbors=7)

Простое дерево показало хорошее разделение между классами по ROC-AUC и accuracy, но метод ближайших соседей оказался лучше.

n_neighbors=7 позволило избежать переобучения — модель не стала слишком сложной.

В данных есть влиятяельные признаки (job_satisfaction_rate), по которым дерево уверенно делит выборку.

## Общий вывод

**Общий вывод**

Целью проекта было решение двух взаимосвязанных задач на основе данных о сотрудниках компании:

**Задача регрессии** — спрогнозировать уровень удовлетворённости работой (`job_satisfaction_rate`) по шкале от 0 до 1.

**Задача классификации** — предсказать, уволится ли сотрудник (целевой признак `quit`, значения: `yes`/`no`).

***

**Этапы работы**

- Загрузка и изучение данных
- Проведён исследовательский анализ данных
- Построены визуализации по признакам, выявлены взаимосвязи
- Предобработка
- Пропуски были обработаны
- Категориальные признаки закодированы с помощью `OneHotEncoder` и `OrdinalEncoder`.
- Числовые признаки масштабированы (`MinMaxScaler`, `StandardScaler`).
- Построение моделей
- Использованы модели
- Проведён подбор гиперпараметров с использованием `GridSearchCV`.
- Качество моделей оценивалось с помощью метрик:`accuracy`, `roc_auc`, `SMAPE`, `MAE`, `r2`

***

**Результаты**

**Регрессия (удовлетворённость работой)**:

  * Лучшая модель:  DecisionTreeRegressor(max_depth=11, max_features=12, random_state=42))
  * Метрика SMAPE ≈ 14% (модель достигла целевого значения ≤ 15)
  * r2 = 0.85 показывает, что модель объясняет большую часть дисперсии
  * Вывод: предсказание точного уровня удовлетворённости возможно, ошибок не много, для предотвращения нежелательных увольнений и выгораний сотрудников - модель необходимо использовать и своевременно обновлять


**Классификация (уволится ли сотрудник)**:

  * Лучшая модель: KNeighborsClassifier(n_neighbors=7)
  * Метрика ROC-AUC: высокая 0,91 (модель достигла значения => 0,91)
  * Вывод: важными факторами являются - уровень удовлетворённости, отдел и нагрузка, необходимо обращать на это внимания и учитывать при подборе сотрудников.


***

**Рекомендации для бизнеса**

* Использовать модель классификации для раннего выявления сотрудников с высоким риском увольнения.
* Обратить внимание на группы с высокой нагрузкой и низкой удовлетворённостью — потенциальная точка риска.
* Периодически проводить внутренние опросы для уточнения удолетворенности и пополнения данных.
* Для улучшения прогноза удовлетворённости рассмотреть:

- Введение метрик вовлечённости, обратной связи с руководством
- Учет изменений в карьерном росте, премиях и графике
"""