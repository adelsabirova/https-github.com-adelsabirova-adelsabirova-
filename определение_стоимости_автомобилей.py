# -*- coding: utf-8 -*-
"""Определение стоимости автомобилей

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zRuyzS1z6ljXM87hOmIp-U-LMa9bYIih

# Определение стоимости автомобилей
Сервис по продаже автомобилей с пробегом «...» разрабатывает приложение для привлечения новых клиентов. В нём можно быстро узнать рыночную стоимость своего автомобиля. В вашем распоряжении исторические данные: технические характеристики, комплектации и цены автомобилей. Вам нужно построить модель для определения стоимости.

Заказчику важны:

- качество предсказания;
- скорость предсказания;
- время обучения.
**Заголовок:** Определение стоимости автомобилей.
    
**Описание проекта:** Современный мир сложно представить без автомобилей. Поэтому Сервис «...» разрабатывает приложение для оценки рыночной стоимости автомобилей с пробегом для их доступного приобретения.
    
**Цель исследования:** Разработать и обучить модель машинного обучения, которая сможет точно определять рыночную стоимость автомобиля с пробегом на основе его технических характеристик, комплектации и других параметров.
При этом важно обеспечить:

- Высокое качество предсказаний, чтобы пользователи получали адекватную оценку стоимости.

- Быстрое время предсказания, чтобы пользователи приложения могли мгновенно узнавать цену своего автомобиля.

- Приемлемое время обучения модели, чтобы можно было легко масштабировать или обновлять модель.

**Ход исследования:**
 - Загрузить данные, путь к файлу: autos.csv.
 - Изучить данные. Заполнить пропущенные значения и обработайте аномалии в столбцах. Если среди признаков имеются неинформативные, удалить их.
 - Подготовить выборки для обучения моделей.
 - Обучить разные модели, одна из которых — LightGBM, как минимум одна — не бустинг. Для каждой модели попробовать разные гиперпараметры.
 - Проанализировать время обучения, время предсказания и качество моделей.
 - Опираясь на критерии заказчика, выбрать лучшую модель, проверить её качество на тестовой выборке.
    
**Общий вывод:** резюмирование полученных результатов, формулировка ключевых выводов и рекомендаций.
    
С помощью данного исследования мы стремимся дать всесторонний анализ рыночной стоимости автомобилей с пробегом, что станет началом для дальнейших исследований и разработки автоматизированной системы предсказания.

## Подготовка данных
### Загрузка данных
"""

from google.colab import files
uploaded = files.upload()

!pip install lightgbm
!pip install -U scikit-learn
!pip install phik q
!pip install --upgrade scikit-learn

import phik
import lightgbm as lgb
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import RandomizedSearchCV

plt.rcParams["figure.figsize"] = (7, 7)

RANDOM_STATE = 42
TEST_SIZE = 0.25

data = pd.read_csv('autos.csv')

"""Датасет назван - data для дальнейшей работы

### Изучение данных
"""

def analysis_df (data):
    data.info()
    display(data.isna().sum())
    display(data.duplicated().sum())
    display(data.head(10))
    display(data.shape)
    display(data.describe())

analysis_df(data)

"""С помощью функции изучаем данные.

354369 - строк в датасете.

Пропуски есть в столбцах - VehicleType, Gearbox, Model, FuelType, Repaired - их достаточно много.

Типы данных везде соответсвуют действительности, но для более удобной работы перевдем дату в формат - '%Y-%m-%dT%H:%M:%S'.

NumberOfPictures - имеет только нули в столбце - из чего можно сделать вывод, что это пустой столбец, он неинформативен.

По изучению медианы и стандратного отклонения видно, что необходимо обработать аномалии.

### Обработка данных
**Изменения типов данных**
"""

data['DateCrawled'] = pd.to_datetime(data['DateCrawled'], format='%Y-%m-%d %H:%M:%S')
data['LastSeen'] = pd.to_datetime(data['LastSeen'], format='%Y-%m-%d %H:%M:%S')
data['DateCreated'] = pd.to_datetime(data['DateCreated'], format='%Y-%m-%d %H:%M:%S')

data.info()

"""**Заполнение пропущенных значений**"""

def grouped_an(df, target_col, group_cols):
    df[target_col] = df[target_col].fillna(
        df.groupby(group_cols)[target_col]
          .transform(lambda x: x.mode().iloc[0] if not x.mode().empty else np.nan)
    )
    return df

"""Создаем функцию для поиска идентичных типов кузова, моделям и тд по бренду и лошадиным силам изучаемой машины. Конечно, нельзя на 100% утверждать, что у одной и той же марки автомобиля не будет одинаковой мощности, но это даст нам больше данных для работы, чем просто заменить на 'unknown' или удалить."""

data = grouped_an(data, 'Model', ['Brand','Power'])
data = data.dropna(subset=['Model'])
data = grouped_an(data, 'VehicleType', ['Model', 'Brand', 'Power'])
data = grouped_an(data, 'FuelType', ['Model', 'Brand', 'Power'])
data = grouped_an(data, 'Gearbox', ['Model', 'Brand', 'Power'])

data.isna().sum()

"""Большинство пропущенных значений удалось заполнить этим методом. Те значения, которые не удалось восстановить и которые составляют незначительную долю данных, были удалены, так как их влияние на итоговый результат будет минимальным.

Пропуски в столбце Repaired являются более важными - оставляем значение 'unknown' для дальнейших исследований. Хотя здесь тоже можно предположить, что если не указано ремонтировалась машина или нет - то ответ "нет"
"""

data = data.dropna(subset=['FuelType', 'VehicleType', 'Gearbox'])

data.isna().sum()

data['Repaired'] = data['Repaired'].fillna('unknown')

"""### Обработка аномалий"""

data.describe()

data['Price'].sort_values().head(11000)

"""По анализу данных видно, что цена машин начинается с 0 евро - что противоречит задаче, ведь сервис продает машины, а не отдает даром. Все таки даже сильно пожившую машину нельзя купить меньше 100 евро - поэтому редактируем столбец, огрничивать верхнюю границу не буду - машину за 20 тысяч евро можно продать/купить."""

data = data[data['Price'] > 100]

"""Следующий столбец дата регистрации - она не может быть больше 2016 по дате DateCrawled, нижнюю границу возьмем 1950 - для ретро моделей. Ниже приведение даты регистрации к непротиворечащей дате выгрузки анкеты."""

data[data['RegistrationYear'] > data['DateCrawled'].dt.year].head()

data.loc[data['RegistrationYear'] > data['DateCrawled'].dt.year, 'RegistrationYear'] = data['DateCrawled'].dt.year

data['DateCrawled'].dt.year.max()

data = data[data['RegistrationYear'] > 1950]
data = data[data['RegistrationYear'] < 2016]

"""Максимальная мощность машины на данный момент - это 4 тысячи л.с., а самые минимальные - 30, будем отталкиваться от этих значений"""

data = data[data['Power'] > 30]
data = data[data['Power'] < 4000]

"""Месяц регистрации можеть быть от 1 до 12, редактируем также как предыдущие столбцы:"""

data = data[data['RegistrationMonth'] >= 1]
data = data[data['RegistrationMonth'] <= 12]

data['RegistrationMonth'].unique()

"""Почтовый индекс оставим без изменений, а вот неинформативный столбец 'NumberOfPictures' удалим. Данные по датам активации и дате скачивания анкеты тоже будут неинформативными, данных по году и месяцу регистрации машины вполне достаточно.

### Удаление столбцов
"""

data = data.drop(['DateCrawled', 'NumberOfPictures', 'LastSeen', 'DateCreated'], axis=1)

analysis_df(data)

data.duplicated().sum()

data = data.drop_duplicates().reset_index(drop=True)

data.info()

"""### Вывод
Для дальнейшей работы датасет был загружен под названием 'data'.
В исходных данных — **354тысячи строк**.

Были выявлены пропуски в нескольких важных столбцах: VehicleType, Gearbox, Model, FuelType, Repaired.

Столбец NumberOfPictures оказался полностью пустым (содержит только нули) и был удален как неинформативный.

Столбцы с датами регистрации пользователей переведены в годы — остальные даты были удалены из-за неиформативности

**Price**: значения с нулевой ценой удалены, так как отдать автомобиль бесплатно практически невозможно; нижняя граница установлена на уровне 100 евро.

**RegistrationYear**: установлены границы от 1950 до 2016 года, чтобы отсеять ошибки ввода.

**Power**: минимальная и максимальная разумная мощность установлена от 30 до 4000 л.с.

**RegistrationMonth**: приведен к диапазону 1–12.

После всех шагов предобработки в датасете осталось **251тысяча строк**, что говорит о потере 30% данных, что не очень хорошо, но работать с медианами/модами даст схожий эффект.

## Обучение моделей
"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# X = data.drop(['Price'], axis=1)
# y = data['Price']
# 
# 
# X_train_valid, X_test, y_train_valid, y_test = train_test_split(
#     X, y,
#     test_size=TEST_SIZE,
#     random_state=RANDOM_STATE
# )
# 
# X_train, X_valid, y_train, y_valid = train_test_split(
#     X_train_valid, y_train_valid,
#     test_size=TEST_SIZE,
#     random_state=RANDOM_STATE
# )
# 
# category = ['VehicleType', 'Gearbox', 'Model', 'FuelType', 'Brand', 'Repaired']
# numeric = ['RegistrationYear', 'Power', 'Kilometer', 'RegistrationMonth', 'PostalCode']
# 
# ohe = OneHotEncoder(drop='first', sparse_output=False, handle_unknown='ignore')
# scaler = StandardScaler()
# 
# X_train_ohe = ohe.fit_transform(X_train[category])
# X_valid_ohe = ohe.transform(X_valid[category])
# X_test_ohe  = ohe.transform(X_test[category])
# 
# X_train_scaled = scaler.fit_transform(X_train[numeric])
# X_valid_scaled = scaler.transform(X_valid[numeric])
# X_test_scaled  = scaler.transform(X_test[numeric])
# 
# X_train_processed = np.hstack([X_train_ohe, X_train_scaled])
# X_valid_processed = np.hstack([X_valid_ohe, X_valid_scaled])
# X_test_processed  = np.hstack([X_test_ohe, X_test_scaled])

"""Ниже выведено время обучения каждой модели и предсказания"""

# Commented out IPython magic to ensure Python compatibility.
model_lr = LinearRegression()
# %time model_lr.fit(X_train_processed, y_train)

# %time preds_lr = model_lr.predict(X_valid_processed)
rmse_lr = np.sqrt(mean_squared_error(y_valid, preds_lr))

# Commented out IPython magic to ensure Python compatibility.
model_rf = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=RANDOM_STATE)
# %time model_rf.fit(X_train_processed, y_train)

# %time preds_rf = model_rf.predict(X_valid_processed)
rmse_rf = np.sqrt(mean_squared_error(y_valid, preds_rf))

# Commented out IPython magic to ensure Python compatibility.
model_lgb = lgb.LGBMRegressor(n_estimators=2500, learning_rate=0.05, num_leaves=40, random_state=RANDOM_STATE)
# %time model_lgb.fit(X_train_processed, y_train)

# %time preds_lgb = model_lgb.predict(X_valid_processed)
rmse_lgb = np.sqrt(mean_squared_error(y_valid, preds_lgb))

# Commented out IPython magic to ensure Python compatibility.
param_distributions = {
    'n_estimators': [100, 200, 500],
    'max_depth': [5, 10, -1],
    'learning_rate': [0.01, 0.05, 0.1],
    'num_leaves': [20, 31, 50],
}

model = lgb.LGBMRegressor()
random_search = RandomizedSearchCV(
    estimator=model,
    param_distributions=param_distributions,
    n_iter=3,
    scoring='neg_root_mean_squared_error',
    cv=2,
    n_jobs=1
)

# %time random_search.fit(X_train_processed, y_train)

random_search.best_params_

-random_search.best_score_

best_model = random_search.best_estimator_

# Commented out IPython magic to ensure Python compatibility.
# %time preds_valid = best_model.predict(X_valid_processed)

rmse_valid_lgb = np.sqrt(mean_squared_error(y_valid, preds_valid))

"""В дополнение к обученным моделям рассмотрим матрицу корреляции и предположим какие столбцы имеют наименее полезную информацию для нашего исследования."""

interval_cols = [
    'Price',
    'RegistrationYear',
    'Power',
    'Kilometer',
    'RegistrationMonth',
    'PostalCode'
]

corr_new = data.phik_matrix(interval_cols=interval_cols)

plt.figure(figsize=(10, 8))
sns.heatmap(corr_new, annot=True, cmap='coolwarm')
plt.title('Корреляция')
plt.show()

"""## Анализ моделей"""

rmse_lr

"""Модель линейной регрессии:

Время обучения модели составило 23 секунды, что соответствует требованию к быстроте модели.

Время предсказания также минимальное, так как линейная регрессия не требует дополнительных вычислений.

Значение RMSE: 2591

Это значение больше порогового требования заказчика (RMSE < 2 500), что говорит о том, что линейная регрессия не подходит к дальнейшей работе.
"""

rmse_rf

"""Модель случайного леса (Random Forest).

Время обучения модели составило 4 минут 41 секунду, что больше по сравнению с линейной регрессией, но все еще не занимает большого количества времени.

Время предсказания тоже невелико.

Значение RMSE: 1 849

Это значение уверенно ниже целевого порога RMSE < 2 500, что удовлетворяет требованиям заказчика, можно использовать для дальнейшйей работы.
"""

rmse_lgb

"""Модель градиентного бустинга LightGBM.

Время обучения модели составило 7 минут 1 секунду, что  больше, чем у линейной регрессии и у рэндомфорест.

Время предсказания тоже в пределах разумного.

Значение RMSE: 1 533

Это значение заметно лучше, чем у остальных моделей и полностью удовлетворяет требованиям заказчика.
"""

rmse_valid_lgb

"""Модель градиентного бустинга LightGBM с тюнингом гиперпарамтеров.

Время обучения модели составило 3 минуты 11 секунду, что больше, чем у линейной регрессии и меньше чем у рэндомфорест.

Время предсказания тоже в пределах разумного.

Значение RMSE: 1 675

Это показатели сильно, чем у остальных моделей и полностью удовлетворяют требования заказчика.

### Итоговый анализ моделей
"""

# Commented out IPython magic to ensure Python compatibility.
# %time preds_test = best_model.predict(X_test_processed)

# %time rmse_test = np.sqrt(mean_squared_error(y_test, preds_test))

rmse_test

"""По итогам анализа на тестовой выборке можно сделать вывод о том, что LGB модель с тюнингом гиперпараметров - best_model - работает быстро и качесвтенное - рекомендую ее к дальнейшему использованию.

## Вывод
По итогам проекта было сделано:

- Исследован датасет: проведён анализ данных, выявлены пропуски и аномалии.
- Обработаны пропуски: пропущенные значения заполнены.
- Удалены неинформативные признаки**: исключён столбец NumberOfPictures и стобцы с датами, так как они неиформативны.
- Обработаны аномалии: для ключевых признаков (цена, мощность двигателя, год регистрации) заданы разумные границы, чтобы избежать некорректных значений, которые могут исказить модель.
- Данные подготовлены для обучения: категориальные признаки закодированы методом OneHotEncoder, числовые стандартизированы.
- Данные разделены на обучающую и тестовую выборки.

Построены и протестированы 3 модели:

1. **Линейная регрессия**

- Быстрая в обучении ~23 секунды
- RMSE: ~2 591
- Качество неудовлетворительное для заказчика (RMSE > 2 500)

2. **Случайный лес**

- Долгое обучение ~4 минут 41 секунд
- RMSE: ~1 849
- Хорошее качество предсказания, но больше времени занимает обучение.

3. **LightGBM**

- Обучение ~7 минут 1 секунд
- RMSE: ~1 533
- Лучшее качество среди протестированных моделей.

4. **LightGBM new - best_model**

- Обучение ~3 минут 11 секунд
- RMSE: ~1 675
- Лучшее качество и скорость среди протестированных моделей.
 LightGBM new продемонстрировали наилучшее качество предсказания (RMSE уверенно ниже порога в 2 500, установленного заказчиком)и быстрое обучение и предсказание, что важно для работы онлайн-сервиса.

Случайный лес же дает и хорошее качество предсказания и неплохую скорость по сравнению с другими моделями.
"""