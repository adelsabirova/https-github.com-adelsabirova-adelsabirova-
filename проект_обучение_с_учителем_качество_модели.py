# -*- coding: utf-8 -*-
"""Проект: Обучение с учителем: качество модели

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1GkFBad5JiNiZ42lNqXYYNTERKTiDi9is

# Проект: Обучение с учителем: качество модели

  **Описание проекта:** Интернет-магазин «...» продаёт разные товары: для детей, для дома, мелкую     бытовую технику, косметику и даже продукты. Интернет-магазин «...» столкнулся со снижением активности покупателей. Поскольку привлечение новых клиентов стало менее эффективным, компания решила сосредоточиться на удержании текущих клиентов через персонализацию предложений.

  **Цель:** разработать систему рекомендаций на основе данных о покупках и поведении пользователей, чтобы      повысить их вовлечённость и частоту заказов через:

  - Анализ клиентской базы и сегментацию.

  - Построение ML-модели для прогнозирования предпочтений.

  - Интеграцию персонализированных предложений в маркетинговые коммуникации.

  **Ход исследования:**
  Шаг 1. Загрузка данных,

  Шаг 2. Предобработка данных,

  Шаг 3. Исследовательский анализ данных,
    
  Шаг 4. Объединение таблиц,

  Шаг 5. Корреляционный анализ,

  Шаг 6. Использование пайплайнов,

  Шаг 7. Анализ важности признаков,

  Шаг 8. Сегментация покупателей,

  Шаг 9. Общий вывод.

  Общий вывод: резюмирование полученных результатов, формулировка ключевых выводов и рекомендаций.

  Результат: рост повторных покупок, увеличение среднего чека и лояльности клиентов.

  Проект актуален, так как персонализация — ключевой тренд, позволяющий повысить конверсию при меньших затратах.

## Шаг. Загрузка данных

### Импортируем библиотеки
"""

from google.colab import files
uploaded = files.upload()

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import shap
import phik
from scipy import stats as st

from sklearn.pipeline import Pipeline
from sklearn.model_selection import train_test_split, RandomizedSearchCV
from sklearn.linear_model import LinearRegression, LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler, MinMaxScaler


from sklearn.metrics import confusion_matrix, classification_report, f1_score, roc_auc_score

from sklearn.impute import SimpleImputer
from sklearn.svm import SVC

plt.rcParams["figure.figsize"] = (7, 7)

RANDOM_STATE = 42
TEST_SIZE = 0.25

"""### Загрузка и изучение данных"""

market = pd.read_csv('market_file.csv', sep=',', decimal='.')
market_money = pd.read_csv('market_money.csv', sep=',', decimal='.')
market_time = pd.read_csv('market_time.csv', sep=',', decimal='.')
money = pd.read_csv('money.csv', sep=';', decimal=',')

def analysis_df (data):
    data.info()
    display(data.isna().sum())
    display(data.duplicated().sum())
    display(data.head(10))
    display(data.shape)

analysis_df(market)

analysis_df(market_money)

analysis_df(market_time)

analysis_df(money)

"""### Вывод

    Первая таблица "market" - 13 столбцов, 1300 строк, дубликатов и пропусков нет.

    Вторая таблица "market_money" - 3 столбца, 3900 строк, дубликатов и пропусков нет.

    Третья таблица "market_time" - 3 столбца, 2600 строк, дубликатов и пропусков нет.

    Четвертая таблица "money" - 2 столбца, 1300 строк, дубликатов и пропусков нет.

    Данный достаточно, чтобы изучить проблему и найти решение.

## Шаг. Предобработка данных

### Работа со столбцами и уникальными значениями
"""

market.columns = market.columns.str.lower()
market_money.columns = market_money.columns.str.lower()
market_time.columns = market_time.columns.str.lower()
money.columns = money.columns.str.lower()

market.head()

market_money.head()

market_time.head()

money.head()

market['id'].nunique()

market_money['id'].nunique()

market_time['id'].nunique()

money['id'].nunique()

market['тип сервиса'].unique()

market['покупательская активность'].unique()

(market['покупательская активность'] == 'Прежний уровень').sum()

(market['покупательская активность'] == 'Снизилась').sum()

market['разрешить сообщать'].unique()

market['популярная_категория'].unique()

market['тип сервиса'] = market['тип сервиса'].replace('стандартт', 'стандарт')

market_money['период'].unique()

(market_money['период']=='препредыдущий_месяц').sum()

market_time['период'].unique()

market_time['период'] = market_time['период'].replace('предыдцщий_месяц', 'предыдущий_месяц')

display(analysis_df(market))
display(analysis_df(market_money))
display(analysis_df(market_time))
display(analysis_df(money))

"""### Вывод

  Явных дубликатов нет, пропусков нет, типы данных совпадают, обработали значения в категориальных столбцах, дабы исключить возможность неявных дубликатов, привели названия столбцов к строчному виду, проверили уникальное кол-во id для дальнейшей работы и выявления ошибок. Убедились, что целевой признак разделен неравномерно. Имеем дело с бинарной неравномерной классификацией.

## Шаг. Исследовательский анализ данных

### Проверка на выбросы в количественных данных
"""

market.drop('id', axis=1).describe()

market_money.drop('id', axis=1).describe()

market_time.drop('id', axis=1).describe()

money.drop('id', axis=1).describe()

"""### Построение графиков для категориальных признаков

#### "market"
"""

category_market = ['покупательская активность', 'тип сервиса', 'разрешить сообщать', 'популярная_категория']

for category in category_market:
    market.groupby(category)['id'].count().plot(
        kind='bar',
        xlabel=category,
        ylabel='Количество покупателей',
        title=f'Количество покупателей по категории: {category}',
        grid = True
    )
    plt.show()

numeric_market = ['маркет_актив_6_мес', 'маркет_актив_тек_мес', 'длительность',
                  'акционные_покупки', 'средний_просмотр_категорий_за_визит',
                  'неоплаченные_продукты_штук_квартал', 'ошибка_сервиса', 'страниц_за_визит']

for column in numeric_market:
    market[column].plot(
        kind='hist',
        bins=20,
        title=f'Распределение: {column}',
        grid = True
    )
    plt.show()

"""#### "market_money"
"""

market_money.groupby('период')['id'].count().plot(
kind='bar',
xlabel='Период',
ylabel='Количество покупателей',
title='Период',
grid = True
);

market_money['выручка'].plot(kind='hist',
bins=20)
plt.title('Выручка')
plt.xlabel('Количество покупателей')
plt.ylabel('Распределение выручки')
plt.xlim(0, 12000)
plt.grid(True)

market_money = market_money[market_money['выручка'] < 100000]

market_money.head(10)

market_money['выручка'].plot(kind='hist',
bins=20)
plt.title('Выручка')
plt.xlabel('Количество покупателей')
plt.ylabel('Распределение выручки')
plt.grid(True)

"""#### "market_time"
"""

market_time.head()

market_time.groupby('период')['id'].count().plot(
kind='bar',
xlabel='Период',
ylabel='Количество покупателей',
title='Период'
);

market_time['минут'].plot(kind='hist',
bins=20)
plt.title('Время')
plt.xlabel('Количество покупателей')
plt.ylabel('Распределение минут')
plt.grid(True)

"""#### "money"
"""

money['прибыль'].plot(kind='hist',
bins=20)
plt.title('Распределение прибыли')
plt.xlabel('Прибыль')
plt.ylabel('Количество покупателей')
plt.grid(True)

money.head()

market_activ = market[market['покупательская активность']=='Прежний уровень']

market_activ.count()

market_money.head()

users_with_zero = market_money[market_money['выручка'] == 0]['id'].unique()

users_money = market_money[~market_money['id'].isin(users_with_zero)]

users_money.nunique()

"""### Вывод

  Было выявлено 1297(802) клиентов, которые совершали покупки в течение трёх и более месяцев.
    
  Проверили датафреймы на наличие выбросов, они были найдены и устранены.

  Построили графики, чтобы посмотреть на количественные и категориальные признак

## Шаг. Объединение таблиц

### Объединение таблиц
"""

market_money_new = market_money.pivot(index='id', columns='период', values='выручка')
market_money_new.columns = [f'выручка_{col}' for col in market_money_new.columns]

market_time_new = market_time.pivot(index='id', columns='период', values='минут')
market_time_new.columns = [f'минут_{col}' for col in market_time_new.columns]

market_merge = market.merge(market_money_new, on='id')
market_merge = market_merge.merge(market_time_new, on='id')

market_merge.head()

"""### Вывод

  Объединили таблицы для дальнейшей работы.
"""

market_merge.isna().sum()

market_merge = market_merge.dropna(subset=['выручка_текущий_месяц'])

"""## Шаг. Корреляционный анализ

### Корреляционный анализ
"""

market_merge_drop = market_merge.drop('id', axis=1)
numeric_market = market_merge_drop.select_dtypes(include='number')

plt.figure(figsize=(15,10))
sns.heatmap(numeric_market.corr(method='kendall'), annot=True, cmap='coolwarm', fmt=".2f")
plt.title('Корреляционная матрица количественных признаков')
plt.show()

corr_new = market_merge_drop.phik_matrix(interval_cols=numeric_market)

plt.figure(figsize=(15,10))
sns.heatmap(corr_new, annot=True, cmap='coolwarm', fmt=".2f")
plt.title('Корреляционная матрица количественных признаков')
plt.show()

"""### Вывод

  Говоря о мультиколинеарности - в нашем датасете она отсутствует. Однако есть более выраженные взаимосвязи признаков:

  - минуты текущего месяца, минуты предыдущего месяца, страниц за визит, неоплаченные продукты за квартал, средий просмотр категорий, акционные покупки, маркетингова активность и покупательская активность.
    
  - акционные покупки и выручка а текущий и предыдущий месяца.
    
  - страниц за визит и минуты текущего и предыдущего месяца.
    
  - выручка предыдущего месяца и выручки текущего и предпредыдущего месяцев.

  В большинстве случаев взаимозависимость данных признаков может нарушить баланс в обучаемой модели.

## Шаг. Использование пайплайнов

### Создание пайплайна
"""

X_train, X_test, y_train, y_test = train_test_split(
    market_merge.drop(['покупательская активность', 'id', 'разрешить сообщать',
                       'минут_предыдущий_месяц', 'страниц_за_визит'], axis=1),
    market_merge['покупательская активность'],
    test_size = TEST_SIZE,
    random_state = RANDOM_STATE,
    stratify =  market_merge['покупательская активность'])

X_train.shape, X_test.shape

ohe_columns = ['популярная_категория']
ord_columns = ['тип сервиса']
num_columns = ['маркет_актив_тек_мес', 'длительность','акционные_покупки', 'минут_текущий_месяц',
               'ошибка_сервиса', 'выручка_предыдущий_месяц', 'выручка_препредыдущий_месяц',
               'выручка_текущий_месяц', 'неоплаченные_продукты_штук_квартал',
               'средний_просмотр_категорий_за_визит', 'маркет_актив_6_мес']

ohe_pipe = Pipeline(
    [('simpleImputer_ohe', SimpleImputer(missing_values=np.nan, strategy='most_frequent')),
     ('ohe', OneHotEncoder(drop='first', handle_unknown='ignore'))
    ]
    )

ord_pipe = Pipeline(
    [('simpleImputer_before_ord', SimpleImputer(missing_values=np.nan, strategy='most_frequent')),
     ('ord',  OrdinalEncoder(
                categories=[['стандарт', 'премиум']],
                handle_unknown='use_encoded_value',
                unknown_value=np.nan
            )
        ),
     ('simpleImputer_after_ord', SimpleImputer(missing_values=np.nan, strategy='most_frequent'))
    ]
)

data_preprocessor = ColumnTransformer(
    [('ohe', ohe_pipe, ohe_columns),
     ('ord', ord_pipe, ord_columns),
     ('num', MinMaxScaler(), num_columns)
    ],
    remainder='passthrough'
)

pipe_final = Pipeline([
    ('preprocessor', data_preprocessor),
    ('models', DecisionTreeClassifier(random_state=RANDOM_STATE))
])

param_grid = [
    {
        'models': [DecisionTreeClassifier(random_state=RANDOM_STATE)],
        'models__max_depth': range(2,5),
        'models__max_features': range(2,5),
        'preprocessor__num': [StandardScaler(), MinMaxScaler(), 'passthrough']
    },

    {
        'models': [KNeighborsClassifier()],
        'models__n_neighbors': range(2,5),
        'preprocessor__num': [StandardScaler(), MinMaxScaler(), 'passthrough']
    },

    {
        'models': [LogisticRegression(
            random_state=RANDOM_STATE,
            solver='liblinear',
            penalty='l1'
        )],
        'models__C': range(1,5),
        'preprocessor__num': [StandardScaler(), MinMaxScaler(), 'passthrough']
    },
    {
        'models': [SVC(kernel='linear', random_state=RANDOM_STATE, probability= True)],
        'models__C': range(1, 5),
        'preprocessor__num': [StandardScaler(), MinMaxScaler(), 'passthrough']
    }
]

randomized_search = RandomizedSearchCV(
    pipe_final,
    param_grid,
    cv=5,
    scoring='roc_auc',
    random_state=RANDOM_STATE,
    n_jobs=-1
)
randomized_search.fit(X_train, y_train)

print('Лучшая модель и её параметры:\n\n', randomized_search.best_estimator_)

randomized_search.best_score_

y_test_pred_one = randomized_search.best_estimator_.predict_proba(X_test)[:, 1]

roc_auc_score(y_test, y_test_pred_one)

y_test_pred_two = randomized_search.best_estimator_.predict(X_test)

np.unique(y_test_pred_two, return_counts=True)

f1_score(y_test, y_test_pred_two, pos_label='Прежний уровень')

display(classification_report(y_test, y_test_pred_two))

cm = confusion_matrix(y_test, y_test_pred_two, labels=['Прежний уровень', 'Снизилась'])

plt.figure()
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=['Прежний уровень', 'Снизилась'],
            yticklabels=['Прежний уровень', 'Снизилась'])
plt.xlabel('Предсказание')
plt.ylabel('Факт')
plt.title('Матрица ошибок')
plt.show()

"""### Вывод
  Создали пайплайн, чтобы найти улчшй модель, убрали признаки с высокой корреляцией и некоторые бесполезные признаки. Мы используем разные кодировщики для категориальных признаков и масштабировщики для количественных.

  Количественные признаки:

  Для обработки количественных признаков мы используем: StandardScaler (стандартизация), MinMaxScaler (масштабирование) и 'passthrough'.

  Категориальные признаки:

  Для категориальных признаков применим два кодировщика: OneHotEncoder (для категорий с несколькими значениями) и OrdinalEncoder (для категориальных переменных с порядковыми значениями).
    
  Лучшей моделью оказалась - LogisticRegression, по метрикам ее показатель вышел на 89.5%.
    
  Использовали метрику roc_auc для неравномерной бинарной классификации и проверили дополнительно метрики f1, recall, precision и матрицу ошибок для визуала.

## Шаг. Анализ важности признаков

### Оценка важности признаков лушчей модели
"""

model = randomized_search.best_estimator_.named_steps['models']
preprocessor = randomized_search.best_estimator_.named_steps['preprocessor']

X_test_transformed = preprocessor.transform(X_test)

feature_names = preprocessor.get_feature_names_out()

X_shap = pd.DataFrame(X_test_transformed, columns=feature_names)

explainer = shap.LinearExplainer(model, X_shap)

shap_values = explainer.shap_values(X_shap)

if isinstance(shap_values, list):
    shap_values_class = shap_values[1]
else:
    shap_values_class = shap_values

shap.summary_plot(shap_values_class, X_shap, feature_names=feature_names)

shap_values.shape

shap.summary_plot(shap_values, X_shap, feature_names=feature_names, plot_type='bar')

coefs = model.coef_[0]
indices = np.argsort(np.abs(coefs))[::-1]

plt.figure()
plt.title('Важность признаков')
plt.barh(range(len(coefs)), coefs[indices], align='center')
plt.yticks(range(len(coefs)), [feature_names[i] for i in indices])
plt.gca().invert_yaxis()
plt.tight_layout()
plt.show()

"""### Вывод
  Для оценки важности признаков для лучшей модели мы использовали метод SHAP.
    
  По графикам и данным выше, можно сделать вывод о том, что и на снижение и на рост покупательской активности сильно влияет макретинговые стратегии и акции. Именно они помогают людям чаще и активнее покупать товары в онлайн-магазине "В один клик".
    
  Также важными факторами являются категория мелкой бытовой техники и выручка за предыдущий месяц, которая определяет покупателя в сегмент покупателей, чья активность снижается.

## Шаг. Сегментация покупателей

### Разделение покупателей на сегменты
"""

market_merge = market_merge.merge(money, on='id')

market_merge.head()

test_ind = X_test.index

market_merge.loc[test_ind, 'предсказание'] = y_test_pred_one

market_merge['прибыль'].describe()

market_merge['предсказание'].describe()

"""Объединяем датафреймы и ставим ограничения по медиане в прибыли - 4.05, и 3 квартиля в значении предсказания - 0.66"""

market_merge_imp = market_merge[(market_merge['прибыль'] > 4.05) & (market_merge['предсказание'] > 0.66)]

plt.figure()

sns.scatterplot(data=market_merge, x='предсказание', y='прибыль',
                label='Другие сегменты', color='purple')
sns.scatterplot(data=market_merge_imp, x='предсказание', y='прибыль',
                label='Исследуемый сегмент', color='blue')

plt.axvline(x=0.66, color='green', linestyle='--')
plt.axhline(y=4.05, color='green', linestyle='--')

plt.xlabel('Вероятность снижения')
plt.ylabel('Прибыль')
plt.title('Анализ клиентов по вероятности снижения и прибыли')
plt.legend()
plt.tight_layout()
plt.show()

market_merge_imp.head()

promo_group = market_merge_imp[market_merge_imp['акционные_покупки'] > 0]
non_promo_group = market_merge_imp[market_merge_imp['акционные_покупки'] == 0]

for row in ['тип сервиса', 'маркет_актив_6_мес', 'маркет_актив_тек_мес', 'популярная_категория', 'средний_просмотр_категорий_за_визит', 'минут_текущий_месяц']:
    plt.figure()
    sns.histplot(non_promo_group[row], color='orange', label='Неакционные', stat='density')
    sns.histplot(promo_group[row], color='blue', label='Акционные', stat='density', alpha=0.5)
    plt.title(f'Сравнение по признаку: {row}')
    plt.legend();

"""По графикам выше мы видим:
- только акционные товары приоретают покупатели премиум сервиса
- стандарт сервис покупает больше неакционные товары, чем акционные
- по маркетинговой активности за 6 месяцев неакционные товары покупают при показателях 4.2-5.2
- по маркетинговой активности в текущем месяце данные практически совпадают - большинство покупок неакционных товаров приходится на 4.5-5.5
- по признаку популярная категория - домашний текстиль обычно пкупают не по акции, все ж остальные товары покупателя стараются купить по скидке
- чем больше средний визит просмотра категория - тем больше покупок не по акции сделали покупатели
- неакционные товары в среднем покупают больше при нахождении в приложении 10-12 минут, распределение акционных товаров находится от 4 до 20 минут включительно

Рекомендации:

Усиливать персонализированные акции и эксклюзивные предложения для премиум-клиентов — это может повысить частоту покупок и лояльность.

Улучшить качество товара, а не только расширение скидок.

Необходимо сфокусироваться на удержании и информировании клиентов: пуш-уведомления, лояльность, сотрудничества с новыми брендами.

Поддерживать равномерный календарь акций, не перегружать пользователей.

Для домашнего текстиля можно сократить глубину скидок и сфокусироваться на премиум-коммуникации. Для остальных — развивать акции и кросс-категорийные промо.

### Увеличение покупательной способности
"""

X_test['предсказанная_активность'] = y_test_pred_two

median_profit = X_test['выручка_текущий_месяц'].median()
X_test['уровень_прибыли'] = 'Низкая'
X_test.loc[X_test['выручка_текущий_месяц'] >= median_profit, 'уровень_прибыли'] = 'Высокая'

X_test['сегмент'] = X_test['предсказанная_активность'] + ' / ' + X_test['уровень_прибыли']
target_group = X_test[X_test['сегмент'] == 'Снизилась / Высокая']

loyal_group = X_test[X_test['сегмент'] == 'Прежний уровень / Высокая']

future_group = X_test[X_test['сегмент'] == 'Прежний уровень / Низкая']

low_group = X_test[X_test['сегмент'] == 'Снизилась / Низкая']

for name, group in X_test.groupby('сегмент'):
    group['выручка_текущий_месяц'].plot(kind='kde', label=name)

plt.legend()
plt.title('Распределение выручки по сегментам')
plt.xlabel('Выручка текущего месяца')
plt.ylabel('Количество покупателей')
plt.show()

X_test['сегмент'].value_counts().plot(kind='bar', figsize=(10, 8))

plt.title('Распределение по сегментам')
plt.xlabel('Сегмент')
plt.ylabel('Количество покупателей')
plt.xticks(rotation=0)
plt.grid()
plt.tight_layout()
plt.show()

loyal_group.describe()

target_group.describe()

future_group.describe()

low_group.describe()

X_test[X_test['уровень_прибыли'] == 'Низкая'][['средний_просмотр_категорий_за_визит', 'маркет_актив_тек_мес', 'акционные_покупки', 'маркет_актив_6_мес']].hist()
plt.ylabel('Количество покупателей')
plt.show()

X_test[X_test['уровень_прибыли'] == 'Высокая'][['средний_просмотр_категорий_за_визит', 'маркет_актив_тек_мес', 'акционные_покупки', 'маркет_актив_6_мес']].hist()
plt.ylabel('Количество покупателей')
plt.show()

"""### Вывод

  Для сегментации покупателей мы использовали результаты моделирования и данные о прибыльности покупателей. Мы разделили покупателей на две основные группы: высокая прибыль и низкая прибыль. Это позволило нам выделить сегменты, которые обладают разной покупательской активностью и потенциальной ценностью для бизнеса.
    
  Мы выбрали сегмент с низким уровнем прибыли, поскольку он имеет потенциал для роста и может быть улучшен с помощью целевых маркетинговых и продажных стратегий.

## Шаг. Общий вывод

**Целью данного проекта** была разработка модели машинного обучения для предсказания покупательской активности в интернет-магазине.

  Исходные данные представляли собой 4 таблицы с различными признаками покупателей и их покупок, включая информацию о выручке, акциях, посещаемых категориях товаров.

   **Предобработка данных включала несколько важных этапов:**
    
  - обработали пропущенные значения
  - кодировали категориальные признаки
  - масштабировали числовые признаки в пайплайне
  - Сегментировали покупателей на низкую и высокую от них прибыль

  Для поиска лучшей модели использовалася пайплайн:

  был создан пайплайн с использованием `ColumnTransformer` для обработки количественных и категориальных признаков раздельно, с применением разных методов кодирования и масштабирования.

  были обучены четыре модели классификации:

  - KNeighborsClassifier
  - DecisionTreeClassifier
  - LogisticRegression
  - SVC

       Для каждой модели была проведена настройка гиперпараметров с использованием кросс-валидации и метода RandomizedSearchCV для поиска оптимальных параметров.

  Для оценки качества моделей использовалась метрика roc_auc_score, которая была выбрана в силу её способности учитывать как точность, так и полноту данных.

  Была выбрана лучшая модель - LogisticRegression, которая показала наибольший результат по результатам метрики. Модель работает дотстаточно хорошо, совершает до 11% ошибок.

  Была выделена группа покупателей с низким уровнем прибыли. Это сегмент, который имеет потенциал для роста, и его покупательская активность может быть увеличена с помощью целевых маркетинговых действий.

  В итоге, маркетинговая работа направленная на покупателей с низкой прибылью и на покупателей, чья активность снизилась, даст положительный результат для бизнеса.
"""