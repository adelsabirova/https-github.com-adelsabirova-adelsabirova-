# -*- coding: utf-8 -*-
"""Прогноз оттока клиентов

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1V0VsHaVaNzPg525umAA4eQc6nf78Nls4

# Проект по прогнозу оттока клиентов.
**Описание проекта:** Оператор связи «...» хочет научиться прогнозировать отток клиентов. Если выяснится, что пользователь планирует уйти, ему будут предложены
промокоды и специальные условия. Команда оператора собрала
персональные данные о некоторых клиентах, информацию об их тарифах и
договорах.

**Цель:** Разработать модель прогнозирования оттока клиентов для оператора связи «...», которая позволит заранее выявлять пользователей, планирующих отказаться от услуг.

**Ход исследования:**
- Шаг 1. Загрузка и изучение данных: загрузить датасеты и изучить структуру данных
- Шаг 2. Предобработка данных: обработка пропусков, добавление новых и удаление ненужных признаков
- Шаг 3. Исследовательский анализ данных: статистический анализ признаков и построение графиков
- Шаг 4. Подготовка данных: пайплайн
- Шаг 5. Обучение моделей
- Шаг 6. Выводы

**Общий вывод:** резюмирование полученных результатов, формулировка ключевых выводов и рекомендаций.

С помощью данного исследования мы стремимся дать всесторонний анализ факторов, влияющих на отток клиентов, определить ключевые признаки, построить модель и выработать рекомендации по снижению уровня оттока.
"""

from google.colab import files
uploaded = files.upload()

!pip install scikit-learn phik q
!pip install lightgbm
!pip install catboost

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import time
import phik

from sklearn.ensemble import RandomForestClassifier
from lightgbm import LGBMClassifier
from sklearn.tree import DecisionTreeClassifier
from catboost import CatBoostClassifier
from sklearn.pipeline import Pipeline
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler, MinMaxScaler
from sklearn.metrics import confusion_matrix, roc_auc_score, ConfusionMatrixDisplay
from sklearn.impute import SimpleImputer
from sklearn.compose import ColumnTransformer

plt.rcParams["figure.figsize"] = (10, 10)

RANDOM_STATE = 250825
TEST_SIZE = 0.25

"""## Загрузка данных"""

contract = pd.read_csv('contract_new.csv')
personal = pd.read_csv('personal_new.csv')
internet = pd.read_csv('internet_new.csv')
phone = pd.read_csv('phone_new.csv')

"""### Изучение структуры данных"""

contract.head(10)

"""Первый датасет назван contract, в нем находится информация о договоре, включает в себя 8 столбцов."""

personal.head(10)

"""Второй датасет назван personal, в нем находятся персональные данные клиента, включает в себя 5 столбцов."""

internet.head(10)

"""Третий датасет назван - internet, в нем хранится информация об интернет-услугах, включает в себя 8 столбцов."""

phone.head(10)

"""Четвертый датасет назван - phone, в нем хранится информация об услугах телефонии, включает в себя 2 столбца.

### Вывод
В нашем распоряжении четыре связанных между собой датасета:

contract — содержит информацию о договоре (8 столбцов);

personal — включает персональные данные клиента (5 столбцов);

internet — хранит данные об интернет-услугах (8 столбцов);

phone — отражает сведения об услугах телефонии (2 столбца).

Все таблицы объединяются по общему идентификатору клиента customerID. Эти данные позволяют проанализировать характеристики пользователей, их тарифы, подключенные услуги и срок действия договора, что важно для прогнозирования оттока клиентов и выявления ключевых факторов, влияющих на уход.

## Предобработка данных
"""

def analysis_df(data):
    data.info()
    display(data.isna().sum())
    display(data.duplicated().sum())
    display(data.shape)
    display(data.head(10))
    display(data.describe())

"""Создаем функцию для изучения данных каждого датасета.

### Датасет contract
"""

analysis_df(contract)

"""В датасете 7043 записи и 8 столбцов, пропусков и дубликатов нет. Ниже исправленные типы данных в столбцах 'BeginDate' и 'TotalCharges' для дальнейшей работы. Средняя ежемесячная плата составляет 64.76 при разбросе от 18.25 до 118.75."""

contract['BeginDate'] = pd.to_datetime(contract['BeginDate'])
contract['TotalCharges'] = pd.to_numeric(contract['TotalCharges'], errors='coerce')
contract.info()

"""### Датасет personal"""

analysis_df(personal)

"""Датасет personal содержит 7043 записи и 5 столбцов, пропусков и дубликатов нет. Все признаки категориальные, кроме SeniorCitizen (булевый тип). Около 16% клиентов — пожилые.

### Датасет internet
"""

analysis_df(internet)

"""Датасет internet содержит 5517 записей, 8 столбцов, пропусков 0, дубликатов 0.Все признаки типа object, все бинарные.

### Датасет phone
"""

analysis_df(phone)

"""Датасет phone содержит 6361 записей, 2 столбца, пропусков 0, дубликатов 0.

Все датасеты оказались достаточно чистыми для дальнейшей работы — пропусков и дубликатов не обнаружено. Были исправлены типы данных. Несмотря на различия в размере таблиц (часть клиентов не пользуется интернетом или телефоном), все датасеты были успешно объединены по признаку customerID. Для дальнейшей работы будут созданы новые признаки и удалены лишние.

## Объединение таблиц и добавление признаков
Объединяем таблицы по общему признаку 'customerID', сохраняем всех клиентов, так как не все пользуются доп.опциями.
"""

data = contract.merge(personal, on='customerID', how='outer') \
                 .merge(internet, on='customerID', how='outer') \
                 .merge(phone, on='customerID', how='outer')

analysis_df(data)

data['TotalCharges'] = data['TotalCharges'].fillna(0)

internet_cols = ['InternetService', 'OnlineSecurity', 'OnlineBackup',
                 'DeviceProtection', 'TechSupport', 'StreamingTV',
                 'StreamingMovies', 'MultipleLines']

for col in internet_cols:
    data[col] = data[col].fillna('No service')

data.isna().sum()

"""Создана одна таблиц из 4 датасетов на основе столбца 'customerID'. Таблица названа data. Обработаны пропуски.
## Исследовательский анализ данных
### Создание новых и удаление старых признаков
Далее создаем целевой признак 'end', который обозначает 0 - действующего клиента, 1 - покинувшего клиента.
"""

data['end'] = (data['EndDate'] != 'No').astype(int)

data.head()

"""Создаем еще один новый признак, который будет показывать сколько дней с клиентом заключен договор. И после исчерпывания признаков 'BeginDate', 'EndDate' - удаляем, чтобы не было утечки данных."""

data['EndDate'] = data['EndDate'].replace('No', '2020-02-01')
data['EndDate'] = pd.to_datetime(data['EndDate'])

data['days'] = (data['EndDate'] - data['BeginDate']).dt.days
data = data.drop(columns=['BeginDate', 'EndDate'])
data.head(10)

"""### Анализ распределений"""

plt.figure()
sns.countplot(x=data['end'])
plt.title("Распределение целевой переменной 'end'");

"""По распределению целевой переменной мы видим, что около 1000 клиентов решили уйти от покупки услуг данной компании."""

num_columns = ['MonthlyCharges', 'TotalCharges', 'SeniorCitizen', 'days']
ohe_columns = ['PaperlessBilling', 'PaymentMethod', 'gender', 'Partner', 'Dependents',
               'InternetService', 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection',
               'TechSupport', 'StreamingTV', 'MultipleLines', 'StreamingMovies']
ord_columns = ['Type']

for col in num_columns:
    plt.figure()
    sns.histplot(data, x=col, hue='end', kde=True)
    plt.title(f"Распределение признака {col} по классам");

for col in ohe_columns:
    plt.figure()
    sns.countplot(data=data, x=col, hue='end')
    plt.title(f"Распределение категориального признака {col} по классам")
    plt.xticks(rotation=45);

data_drop = data.drop('customerID', axis=1)
numeric_market = data_drop.select_dtypes(include='number')

corr = data_drop.phik_matrix(interval_cols=numeric_market)

plt.figure()
sns.heatmap(corr, annot=True, cmap='coolwarm', fmt=".2f")
plt.title('Корреляционная матрица data');

"""### Вывод
По распределениям и матрице мы видим, что в данных не осталось признаков с прямой зависимостью к целевой переменной, но есть признаки, которые помогут модели хорошо обучится. Способ оплаты, месячная ставка, полная сумма оплаты, наличие партнера и количество дней пользователя. Эти признаки, несмотря на умеренную корреляцию, дают модели достаточную информативность для построения качественного прогноза. Создание новых признаков тоже дает хорошее развитие событий в будущем обучении модели, данные готовы для дальнейшего преобразования.

## Подготовка данных к обучению
"""

X = data.drop(columns=['end', 'customerID'])
y = data['end']

X_train, X_test, y_train, y_test = train_test_split(
    X,
    y,
    test_size=TEST_SIZE,
    random_state=RANDOM_STATE,
    stratify=y
)

ohe_pipe = Pipeline(
    [('ohe', OneHotEncoder(handle_unknown='ignore', sparse_output=False)
     )])

ord_pipe = Pipeline(
    [('simpleImputer_before_ord', SimpleImputer(missing_values=np.nan, strategy='most_frequent')),
     ('ord',  OrdinalEncoder(
                categories=[['Month-to-month', 'One year', 'Two year']
                           ],
                handle_unknown='use_encoded_value',
                unknown_value=np.nan
            )
        ),
     ('simpleImputer_after_ord', SimpleImputer(missing_values=np.nan, strategy='most_frequent'))
    ]
)

num_pipe = Pipeline([
    ('imputer', SimpleImputer(strategy='median')),
    ('scaler', MinMaxScaler())
])

data_preprocessor = ColumnTransformer([
    ('ohe', ohe_pipe, ohe_columns),
    ('ord', ord_pipe, ord_columns),
    ('num', num_pipe, num_columns)
], remainder='passthrough')

"""Помимо столбца 'end'(целевого признака) в переменной X удаляем столбец 'customerID', так как он не несет никакой информативности

## Обучение моделей
"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# pipe_final = Pipeline([
#     ('preprocessor', data_preprocessor),
#     ('model', CatBoostClassifier())
# ])
# 
# param_grid = [
#     {
#         'model': [DecisionTreeClassifier(random_state=RANDOM_STATE)],
#         'model__max_depth': range(5, 20),
#         'model__max_features': ['sqrt', 'log2'],
#         'preprocessor__num__scaler': [StandardScaler(), MinMaxScaler()]
#     },
#     {
#         'model': [RandomForestClassifier(random_state=RANDOM_STATE)],
#         'model__n_estimators': [50, 100],
#         'model__max_depth': [5, 10],
#         'model__max_features': ['sqrt', 'log2'],
#         'preprocessor__num__scaler': [StandardScaler(), MinMaxScaler()]
#     },
#     {
#         'model': [LGBMClassifier(random_state=RANDOM_STATE)],
#         'model__n_estimators': [100, 200],
#         'model__max_depth': [5, 10],
#         'model__learning_rate': [0.05, 0.1],
#         'model__num_leaves': [31, 63],
#         'preprocessor__num__scaler': [StandardScaler(), MinMaxScaler()]
#     },
#     {
#         'model': [CatBoostClassifier(random_state=RANDOM_STATE, verbose=False)],
#         'model__iterations': [100, 200, 500],
#         'model__depth': [4, 6, 8],
#         'model__learning_rate': [0.01, 0.05, 0.1],
#         'model__l2_leaf_reg': [1, 3, 5],
#         'preprocessor__num__scaler': [StandardScaler(), MinMaxScaler(), None]
#     }
# ]
# 
# grid_search = GridSearchCV(
#     pipe_final,
#     param_grid,
#     cv=5,
#     scoring='roc_auc',
#     n_jobs=-1,
#     error_score='raise'
# )
# 
# grid_search.fit(X_train, y_train)
# 
# print("Лучшие параметры:", grid_search.best_params_)
# print("Лучший ROC-AUC:", grid_search.best_score_)
#

best_model = grid_search.best_estimator_

"""Лучшей моделью стала - CatBoostClassifier с параметрами:

'model__depth': 4,
'model__iterations': 500,
'model__l2_leaf_reg': 1,
'model__learning_rate': 0.1,
'preprocessor__num__scaler': None
"""

y_pred = best_model.predict(X_test)
y_pred_proba = best_model.predict_proba(X_test)[:, 1]

roc_auc = roc_auc_score(y_test, y_pred_proba)
roc_auc

cm = confusion_matrix(y_test, y_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=best_model.classes_)
disp.plot(cmap='Blues')
plt.title("Матрица ошибок");

"""Матрица ошибок показывает нам, что модель ошибается не слишком часто и компания не теряет на этом крупную сумму денег.

## Итог

- Данные были загружены и изучены.
- Обнаружены и обработаны пропуски.
- Приведены типы данных к корректным форматам.
- Целевая переменная — 'end' (отток клиентов).

---

- Несколько источников(4 таблицы) были объединены в единый датафрейм для дальнейшего анализа.
- Проведён анализ распределений числовых и категориальных признаков.
- Выявлены ключевые признаки, которые влияют на отток:

  способ оплаты,
  месячная ставка,
  общая сумма оплаты,
  наличие партнёра,
  количество дней использования.
  
  
- Проверена корреляция: прямых сильных связей с целевой переменной нет, но признаки обладают информативностью.
- Сгенерированы дополнительные признаки (например, длительность подписки).
- Числовые признаки масштабированы (MinMaxScaler / StandardScaler).
- Категориальные признаки закодированы (OneHotEncoder, OrdinalEncoder).
- Данные разделены на обучающую, валидационную и тестовую выборки.

---

- Были протестированы несколько алгоритмов: Decision Tree, Random Forest, LightGBM и CatBoost.
- Настроен пайплайн с GridSearchCV для подбора гиперпараметров.
- Наилучшие результаты показала модель **CatBoostClassifier**.

- Лучший ROC-AUC на кросс-валидации: 0.91.
- Результаты:

  - Тестовая выборка: ROC-AUC = 0.91
- Время обучения: около 40 минут.

---
- Разработанная модель способна с высокой точностью (ROC-AUC > 0.9) прогнозировать отток клиентов.
- Это позволит бизнесу заранее выявлять клиентов с высоким риском ухода,разрабатывать персональные предложения (скидки, бонусы, программы лояльности). Рекомендуется внедрить модель в рабочий процесс и регулярно обновлять обучение по мере накопления новых данных.
"""